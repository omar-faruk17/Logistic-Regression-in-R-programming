# The concept of model fit is taken directly from Logistic Regression in R for Public Health by Imperial College London

There are essentially two very different ways of approaching this question: predictive power and goodness of fit. Ideally, you want your model to do well on both. Some of what I'll cover is also relevant to other types of regression, and the course on linear regression in this series introduced the important concept of the residual.

With the first, the aim is to get a statistic that measures how well you can predict the dependent variable (the outcome, so getting diagnosed with diabetes in our case) based on the independent variables (the predictors, such as age and BMI). This will tell you about the “predictive power” or “explanatory power” of the model. Generally they range between 0 (the model explains none of the data and the variables don’t predict the outcome at all) and 1 (the variables predict the outcome completely). These include measures such as the R-square and the area under the ROC curve, which we’ll look at shortly.  

The other approach to evaluating model fit is to compute what’s called a goodness-of-fit statistic. These kind of measures include the deviance and the popular Hosmer-Lemeshow statistic. There are formal tests for these that yield a p value, so if you’re happy to use the usual cut-off of p=0.05, you can use them to decide whether your model fits the data acceptably. Again, more on those shortly. Goodness of fit tells you nothing about predictive power – and vice versa. You can get good prediction with poor fit or a model with good fit but poor prediction.



# R-squared measures
The previous course on linear regression models covered how to test how well your linear regression model fits the data. The most common way to do so is with the R-squared value, which measures the proportion of the variance in the outcome variable (Y) that can be explained by your predictor variables (X1, X2… etc). An R-squared value close to 1 indicates strong predictive power, while one close to 0 indicates poor predictive power. As you now know, logistic regression is used when the outcome variable is binary. We can't do correlation tests if your Y can only take 2 values – so what can we do?

It turns out that there are many ways to approximate an R-squared for logistic regression. One of the best ways is with the McFadden (pseudo) R-squared. This measure depends on the “likelihood” of your model, which is a cryptic way of describing how compatible your model parameters are with the observed data. You don’t need to know the details of the calculation of the likelihood or indeed of McFadden’s R-squared measure. The McFadden R-squared can be interpreted in a similar way to the “classic” R-squared from a linear regression: high values are best. In practice, though, R-squared values – whether the McFadden version or any other – tend to be pretty low and certainly lower than people who are used to linear models expect. This does not mean the model is bad – it's more a reflection of the limitation of the R-squared measure than of the model. 



# Discrimination: c statistic or area under the ROC curve
In prognostic modelling – estimating the risk of an outcome (e.g. disease) based on a person’s characteristics (e.g. age, gender, etc) – we want to be able to assess a model’s discrimination. Discrimination is a “measure of how well the model can separate those who do and do not have the [outcome] of interest” [Nancy Cook in Circulation 2007]. In our case, we’re interested in distinguishing between people with and without a disease (diabetes). When looking at a sample of patients, a model with good discrimination will declare those with a disease to have had a higher risk than all of those without the disease. Therefore, in the modelling world, discrimination is a good thing.

You can see why you would want to test this for your logistic models. In the example you’ve done, you've built a model to test the potential relationship between several traits (age, gender, cholesterol level, etc) with a disease outcome: diabetes. In your sample of patients, would your model predict a higher risk score for those who we have observed to have diabetes than those who don’t?

One of the most popular ways to do this is called the “area under the receiver operating characteristic (ROC) curve, or “c-statistic” for short. The ROC is a plot of sensitivity (probability of a positive result among the cases) against 1 - specificity (probability of a negative result among the non-cases). A “case” here is someone with the disease or outcome of interest. This can be reworded as the “true positive rate” vs. the “false positive rate”. 

The area under a curve, which is calculated by a technique called integration, is the c-statistic. A c-statistic of 0.5 indicates that the model is only as good at predicting the outcome as random chance (i.e. no discrimination). A curve at or close to the black line (y=x) in the diagram would be an example of this. As the curve pulls away and above from the black line, the area under it increases, so therefore the discrimination increases. In the diagram, the model represented by the red ROC curve has the best discrimination. A c-statistic of 1 would be perfect, but of course this never happens in real life and in fact, as Cook’s article shows, the theoretical maximum for a given model is often lower than this. A c-statistic below 0.5 would predict the outcome worse than random chance, which would mean a very poor model indeed.



# Deviance
This word has certain connotations in non-statistical spheres, but in regression it concerns how well – or rather how badly – the model fits the data. It’s a measure of the “goodness of fit”. In a linear model, where the outcome can take on any value, the predicted value can match the actual outcome exactly or differ from it by a measurable amount. This leads straightforwardly to the concept of deviance – a measure of how the prediction differs from the observed outcome. In logistic regression, however, the observed outcome can only take on two values, zero and one, whereas the predicted value, a log odds, can take on any value and can be mapped to a probability, which can take on any value between zero and one. Therefore, we can’t just take the deviance measure from the linear regression case. Some adjustment is necessary. 

One very common approach can be taken when the data can be aggregated or grouped into unique “profiles”: groups of cases that have exactly the same values on the predictors, e.g. patients with the same age, gender and insurance. After fitting the model, we can get an observed number of events and an expected number of events for each profile. The two well-known statistics for comparing the observed number with the expected number are the deviance and Pearson’s chi-square. Both produce statistics that can be compared against tables of a chi-squared distribution in order to see how unusual the value of the statistic is for that model, which yields a p value. High p values (e.g. above the usual threshold of 0.05) mean that the model’s deviance statistic is nothing unusual, which is a good thing as it means that the model fits the data well. The deviance compares our model with a few variables in against one that fits the data perfectly – the “saturated model” – to see whether we’re missing anything important, such as interactions between variables or non-linearities.

This profile approach is fine when we have only categorical predictors. It will likely be fine with age (in years) if there are a number of patients with each different value for age. If the data are spread so that there’s only one case per profile, then the deviance and the Pearson’s chi-square statistic won’t fit the chi-squared distribution very well at all, so the test breaks down. What can we do? This leads us to the next measure: the Hosmer-Lemeshow statistic, proposed in 1980 and still very much in use today. 

# Calibration: Hosmer-Lemeshow statistic and test
Here, patients are grouped together according to their predicted values from the model, which are ordered from lowest to highest and then separated into typically ten groups of roughly equal size. For each group, we calculate the observed number of events – here that’s the number of patients with diabetes – and the expected number of events, which is just the sum of the predicted probabilities for all the patients in the group. Pearson’s chi-square test is then applied to compare observed counts with expected counts. A large p value (e.g. above the conventional 0.05) indicates that the model’s predicted values are a good match for the real (observed) values, i.e. the model is a good fit.

The authors’ own work has revealed some limitations with the test. With small data sets, the test has limited ability (limited power) to detect important differences between the observed and expected counts i.e. to detect poor fit that’s poor enough to worry about. At the other end, with large data sets, you can get a low p value when the difference between the observed and expected isn’t that important.

It has some other issues too. One is that although the standard number of groups to use is ten, the p value can be altered just by choosing fewer or more than ten groups, and there’s no good way of deciding on the number. Another is that some people, including me, have found that adding interaction terms between variables, even non-significant ones, can alter the statistic.

# How to get these statistics in R
The deviance is given by default – see the next item on the course – but the R-squared, c statistic and Hosmer-Lemeshow statistics and test have to be requested in R.

# McFadden’s r-squared: In the formula where lnL(MFull) is the (maximised) log likelihood of your fitted model and lnL(MNull) is the log likelihood of a null model. You can calculate this is a simple way manually in R.


# Null Deviance and Residual Deviance
To understand residual deviance, we must first think about 3 models: the null model, the proposed model and the saturated model. The null model, as discussed earlier, is one where we only include the intercept: this model therefore only has one parameter. The proposed model is the model with the variables we included in our logistic regression. The number of parameters in the proposed model is the number of variables plus one (the intercept): this is because each of the variables we have included only needs one parameter, but remember that a categorical variable with three categories, for example, will need two parameters. The saturated model is a model which fits the data perfectly, because it has as many parameters as there are data points.

The null deviance is a measure of how well the null model explains the data compared with the saturated model. Having just one parameter, the null model does not usually explain the data very well, and this is indicated by a large null deviance. The point of doing a regression model is that we reckon we can do better at explaining the data with a few variables (age, sex, etc). This brings us on to the residual deviance: how well the proposed model explains the data compared with the saturated model. 

The difference between the null deviance and the residual deviance gives us an idea of how well our model has performed (at the cost of degrees of freedom). It’s like a return on investment: how much benefit (explanation of the variation in the outcome) do we get for our investment (the variables we’ve added or, more accurately, the degrees of freedom taken up by those variables). If the model is “good”, then the difference between the null deviance and the residual deviance will be large. There are formal ways to see whether the difference is large enough.

# AIC
Lastly, I’ll mention the AIC. This is short for Akaike Information Criterion and measures the quality of a model in terms of the amount of information lost by that model. It therefore recognises that all models lose information compared with “reality” but some models lose less than others. It’s of no use by itself but is used for comparing two or more models. Small AIC values are best.


# Tjur T. Coefficients of determination in logistic regression models—A new proposal: The coefficient of discrimination. The American Statistician 2009; 63: 366-372.
The definition of this one is very simple, so it’s easy to calculate in R. For each of the two categories of the dependent variable, so having diabetes yes / no, calculate the mean of the predicted probabilities of having diabetes. Then take the difference between those two means. Tjur called this the coefficient of discrimination, and you can interpret it like any other R-squared as the proportion of variation in the outcome that’s explained by the model. High values are best. 


#......................................................................................
# Summary of Different Ways to Run Multiple Regression, Ways to choose your model
#......................................................................................
As I explained in the videos, the problem is that having too few predictors leads to poor prediction, but having too many can cause overfitting, non-convergence, difficulty in interpretation and explaining results to other people. So how do you choose the predictors?

It’s always a good idea to begin by reviewing the relevant literature and expert knowledge, but that will only get you so far. It may tell you that you should include age, gender and perhaps a few other things, but it’s very likely that you’ll have other variables in your data set that are worth trying. One option is to enter and keep them all in your model, whatever the p values.  This is a good idea if you don’t have too many and / or you can use a priori knowledge for them all.  (but beware when this prior knowledge comes from studies using poor methods)

A variant of this is backwards elimination, where you then drop the non-significant variables. This works OK in some circumstances, but you need to check for correlation between variables. The best way to do that is by inspecting the odds ratios for the predictors that you are keeping – first with all variables in the model and second when you drop some. If the odds ratios change noticeably when you drop some, then you’ll need to add back at least one of the dropped ones.

Forward selection involves starting with an empty model and trying variables one at a time. Stepwise selection involves a mixture of forwards and backwards. Both of these should be avoided. Similarly, “all-possible-regressions", which literally tests all possible subsets of the set of potential independent variables, is to be avoided. It might sound like it, but it’s not in fact guaranteed to give you the best model for your data.  

Machine learning approaches are increasingly popular, but are complex and out of our scope.

Training and testing data sets

It’s good practice to split your data set if possible into a training and a testing data set and apply the training-set model to the test-set data. If you get very different answers, then rethink the complexity of training-set model and repeat. This is standard practice with machine learning because of the risk of overfitting and "overtraining", in which the algorithm fits the data too closely so it's unable to distinguish between signal and noise in the data set and so performs badly on a new data set. However, it's also strongly recommended with statistical models where possible. Another related technique is called k-fold cross-validation, which is often used when you have limited data.

With small data sets, splitting into a training and a testing set is not advisable. The data sets we are using in these statistics for public health courses are considered small, which is why I haven’t suggested splitting them. If you're keen to go beyond this introductory course, then I suggest learning about cross-validation. If you're taking our Global Master's degree in Public Health, then it will be covered in the Advanced Statistics specialisation, which will also cover things like LASSO and elastic nets that are relevant to model selection.


# Further Reading on Model Selection Methods
There’s more detail on how stepwise selection is done and why it’s a bad idea at 
https://people.duke.edu/~rnau/regstep.htm
 You can’t trust the coefficients and you can’t trust the p values. That’s true for all types of regression, including Cox that we’re covering in the next course in this specialisation.

Rather than use p values for each variable to decide whether to include it, a (better) alternative is to be guided by a quantity that I mentioned earlier called the AIC, Akaike’s Information Criterion, or a similar one called the Bayes Information Criterion (BIC). Without going into the maths behind the “information” (which has a technical meaning here), you just need to know that the AIC aims to describe how well the model fits the data while penalising models with lots of coefficients and that lower values of the AIC are desirable. It's useful for comparing models, but if you only run one model it's of no value.

Some people use the R-squared statistic, which you read about earlier in this course, to pick the “best” model. The R-squared estimates the predictive power of the model and tells you nothing about how well the model fits the data (goodness of fit). It’s not ideal but it does give you useful information. With logistic regression, the maths behind the R-squared is trickier than for linear regression, leading to many different versions of it being proposed. I'd advise against using only the R-squared to pick your best model. 

An alternative to choosing a single model is to use model averaging. This has a different philosophy from the model selection methods by considering that there are several “good” candidate models and that we don’t actually have to choose between them – we can just take an average. This idea has been developed and applied to all kinds of regression. This has been quite a large area of methodological research for some time, and model averaging can be done in R with functions written by users. It’s now often used to average regression coefficients across multiple models with the ultimate goal of capturing a variable's overall effect. This use of model averaging implicitly assumes the same parameter exists across models so that taken an average is a sensible thing to do. At first glance, this assumption seems reasonably, but regression coefficients associated with particular variables might not have the same interpretations across all of the models in which they appear, and that makes interpreting the averaged value tricky. Despite the issues – after all, there are issues with every method in existence – model averaging is widely used, but the size of the literature and the technical details of Bayesian model averaging in particular mean that I’ll go no further. A readable summary of the area is given by 
https://warwick.ac.uk/fac/sci/statistics/crism/research/17-06/17-06w.pdf
 The focus is on Bayesian model averaging – in Bayesian statistics, one combines the data with one’s prior beliefs to produce the model output, whereas in classical or “frequentist” statistics, one is driven only by the data. The article also covers the frequentist approach. There are formulae, which you can skip over if you like, and the author is an economist, but it’s still a very readable account of the subject. 

